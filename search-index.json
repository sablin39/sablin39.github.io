[{"title":"About ADHD Symptoms and Hormones","date":"2025-01-16T13:29:07.000Z","url":"/healthcare/adhd-hrt-notes.html","tags":[["Life with ADHD","/tags/Life-with-ADHD/"],["HRT","/tags/HRT/"]],"categories":[["healthcare","/categories/healthcare/"]],"content":"Written after I fucked up my therapy due to gastrointestinal disorder as a remainder. Don’t take it seriously. While male with ADHD often have noticeable symptoms like hyperactivity or disruptive behavior, females with ADHD are often characterized by inattention, forgetfulness, and internalized behaviors (e.g., anxiety, depression). This can lead to females being overlooked or misdiagnosed with other conditions like anxiety or depression. Interestingly, symptoms observed from transgender individuals can be consistent with their gender identity even before Hormone Replacement Therapy (HRT). Effects of Female HormonesThe change of estrogen and progesterone is highly related to ADHD symptoms. The symptoms are consistent with menstrual cycle, making it more acute when experiencing premenstrual syndrome (PMS) and premenstrual dysphoric disorder (PMDD). Effects for hyperactivity-impulsivity appeared mainly driven by declines in estrogen regardless of levels of progesterone, consistent with more of an ovulatory effect. In contrast, inattentive effects were driven by declines in estrogen but were moderated by levels of progesterone.1 In the follicular phase, which is the period when estrogen levels are steadily increasing, ADHD symptoms are at their lowest. 2 While ADHD risk appears to be highest for males during the prenatal period, females experience heightened risk during multiple reproductive periods. Adolescents are particularly vulnerable to the impact of hormones and environmental stressors. This may lead to a structural change in brain, causing a rising possibility of triggering ADHD symptoms. Evidences shown that many female individuals are diagnosed after adolescence, while they may have minimal or no symptoms suggesting ADHD. Interestingly, transgender female may be diagnosed as adult ADHD after they started Hormone Replacement Therapy (HRT). Changes in response to ADHD medications likely reflect the point in the menstrual cycle that a woman is and where all these different hormones are. In practice, clinicians may adjust medication and even add additional medication to help women manage their ADHD and period-related exacerbations. Hormonal changes at puberty — especially the higher levels of estrogen and progesterone — can cause ADHD medications to be less effective. Studies have shown that estrogen may enhance a woman’s response to amphetamine medications, but this effect may be diminished in the presence of progesterone.3 Additionally, The loss of estrogen leads to a decrease in serotonin and dopamine levels in the brain leads to a higher rate of moodiness, sadness, irritability, fatigue, fuzzy thinking, and memory lapses.4 A combination of HRT with ADHD medication often improves symptoms most effectively. Effects of TestosteroneADHD exhibits a prominent sex-biased prevalence rate in childhood with a 3:1 ratio favoring boys. 5 Testosterone plays a crucial role in attention, focus, and inhibitory control, functions that are often impaired in ADHD. However, studies also suggest that testosterone supplementation may alleviate some symptoms, such as hyperactivity and impulsivity. Cognitive control was impaired in girls with mild and severe ADHD (Inattentive and Combined subtypes of ADHD), but only in boys with severe ADHD (Combined ADHD subtype), suggesting that girls needed more severe impairment in order to exhibit symptoms. 6 A steady testosterone level leads to improved focus, better emotional regulation, and increased body strength.8 This is obviously beneficial to ADHD symptoms since the major way to reduce the effect of ADHD is Mindfulness-Based Cognitive Therapy (MBCT) and exercise. High testosterone level may worsen hyperactive and impulsive symptoms, leading ADHD boys being “too energetic” but easier to be discovered and diagnosed. Hypothesis suggests that prenatal testosterone exposure may have potential effects on ADHD symptoms. A survey conducted by 7 indicated that higher fetal testosterone levels may play a role, not only in the origin of autism, but also in the aetiology of PDD-NOS and of ADHD&#x2F;ODD. Males with anxiety disorders might have been exposed to lower prenatal testosterone levels. Evidence from Turner’s Syndrome also suggests that higher testosterone:estrogen ratios relate to cognitive control problems similar to those seen in ADHD. Symptoms of Transgender Individuals No reliable resources available. Changes in hormones during puberty is also vital for brain circuitry development. For example, several types of reproductive and social behaviors (e.g., sexual behavior, scent marking, social interactions) are impaired in male rodents if they are castrated before puberty.9 These changes may result in organizational effects on the brain, and hormone replacement in adulthood does not reverse these deficits. 8 Transgender indivudals are reported to have a higher rate of having ADHD, ASD than their peers, which may resulted from social pressure or changes in hormone levels. The link between sex hormones to ADHD symptoms is still not clear, it can effect structural development of brain during puberty and controls our daily behaviors. Even hormone exposure levels during pregnancy may be a vital factor. Most of the effects presented are just having “correlation” instead of “causality” with hormones. To sum up, DO NOT mess up any medication for any reason. References1.Eg, A.G., Nirjar, U., Elkins, A.R., Sizemore, Y.J., Monticello, K.N., Petersen, M.K., Miller, S.A., Barone, J., Eisenlohr-Moul, T.A., &amp; Martel, M.M. (2024). Attention-deficit/hyperactivity disorder and the menstrual cycle: Theory and evidence. Hormones and Behavior, 158(105466). ISSN 0018-506X. ↩2.Broadway, C. (2024, April 19). High and low estrogen exacerbate ADHD symptoms in females: New theory. ADDitude. ↩3.Roberts, B., Eisenlohr-Moul, T., &amp; Martel, M. M. (2018). Reproductive steroids and ADHD symptoms across the menstrual cycle. Psychoneuroendocrinology, 88, 105–114. ↩4.Weber, M. T., Maki, P. M., &amp; McDermott, M. P. (2014). Cognition and Mood in Perimenopause: A Systematic Review and Meta-Analysis. The Journal of steroid biochemistry and molecular biology, 142, 90–98. ↩5.American Psychiatric Association (2000). Diagnostic and Statistical Manual of Mental Disorders Fourth edition Text Revision (DSM-IV-TR). Washington DC American Psychiatric Association. - References - Scientific Research Publishing. (n.d.). ↩6.Nigg, J. T., Blaskey, L. G., Huang-Pollock, C. L., &amp; Rappley, M. D. (2002). Neuropsychological executive functions and DSM-IV ADHD subtypes. Journal of the American Academy of Child &amp; Adolescent Psychiatry, 41(1), 59–66. ↩7.De Bruin, E. I., Verheij, F., Wiegman, T., &amp; Ferdinand, R. F. (2006). Differences in finger length ratio between males with autism, pervasive developmental disorder–not otherwise specified, ADHD, and anxiety disorders. Developmental Medicine &amp; Child Neurology, 48(12), 962. ↩8.Martel, M. M., Klump, K., Nigg, J. T., Breedlove, S. M., &amp; Sisk, C. L. (2009). Potential hormonal mechanisms of Attention-Deficit/Hyperactivity Disorder and Major Depressive Disorder: A new perspective. Hormones and Behavior, 55(4), 465–479. ↩9.Schulz, K. M., &amp; Sisk, C. L. (2006). Pubertal hormones, the adolescent brain, and the maturation of social behaviors: Lessons from the Syrian hamster. Molecular and Cellular Endocrinology, 254–255, 120–126. ↩"},{"title":"Brief Dive into Macro-o1 and o1-like LLMs","date":"2024-11-29T05:30:38.000Z","url":"/LLM/macro-o1.html","tags":[["LLM","/tags/LLM/"],["MCTS","/tags/MCTS/"]],"categories":[["LLM","/categories/LLM/"]],"content":"Macro-o1 7B is the first o1-like open source LLM. In this post, we will dive into its training paradigm and its inference implementation. Code link:  Paper link:  Update on 2025&#x2F;04&#x2F;10:Though this model is quite suck compared with GRPO+SFT pipeline as Deepseek-R1, its token-level value inside MCTS is quite interesting. This may potentially be a trick for pretraining small langugage model of different structure than GPT. A token-level reward formulation like this is easy to be derived since it shares the same formulation as perplexity during pretraining. However, it offers too much uncessary information in SFT stage and may cause instability.Always be open to new ideas and value possibilities. No one knows the value of an infant. Training ParadigmMacro-o1 adopts full-param SFT using existing CoT datasets by OpenAI and synthetic dataset generated by open source LLM. Compared with previous SFT, a CoT dataset provides a linear path for LLM to follow, while LLM can also explore other action nodes for exploration. This may lead to a balance between exploration and exploitation, significantly reduces the distribution shift caused by bias within dataset. MCTS Revisit Selection: Start from root R and select successive child nodes until a leaf node L is reached. The root is the current game state and a leaf is any node that has a potential child from which no simulation (playout) has yet been initiated. The section below says more about a way of biasing choice of child nodes that lets the game tree expand towards the most promising moves, which is the essence of Monte Carlo tree search. Expansion: Unless L ends the game decisively (e.g. win&#x2F;loss&#x2F;draw) for either player, create one (or more) child nodes and choose node C from one of them. Child nodes are any valid moves from the game position defined by L. Simulation: Complete one random playout from node C. This step is sometimes also called playout or rollout. A playout may be as simple as choosing uniform random moves until the game is decided (for example in chess, the game is won, lost, or drawn). Backpropagation: Use the result of the playout to update information in the nodes on the path from C to R. Training ProceduresIn the training procedure, “action” is formulated as LLM outputs, or to say, the fixed-length tokens {ti}i=1N\\{t_i\\}^N_{i=1}{ti​}i=1N​. Then the value of each state is obtained by applying the softmax function to its log probability and the log probabilities of the top 5 alternative tokens. ci=exp⁡(p(ti))∑k=15exp⁡(p(tk)) c_i=\\frac{\\exp(p(t_i))}{\\sum^5_{k=1}\\exp(p(t_k))} ci​=∑k=15​exp(p(tk​))exp(p(ti​))​ Here p(ti)p(t_i)p(ti​) is the log probability of token generated by LLM, and p(tk)p(t_k)p(tk​) denotes the top kkk predicted tokens at step iii. Thus the reward of a state is obtained as: v=1N∑i=1Nci v=\\frac{1}{N}\\sum^N_{i=1}c_i v=N1​i=1∑N​ci​ Dataset PreparationBesides Open-O1 dataset, they also built a synthetic dataset using MCTS. An example of CoT data is provided in CoT_demo.json . It’s generated by Qwen2.5-7B-Instruct using the above value formulation for tree search. InferenceMacro-o1 has a vllm implementation based on Qwen2Model. Compared with original Qwen structure, it adds a generate_response function above model’s forward instead of directly use model.generate(). The following is huggingface implementation, which is more detailed than vllm ones. Such generation process exposes the logits of tokens to achieve the above MCTS procedures. Such token-level, or to say, logit-level value formulation is quite interesting and can be seen as a successor of sampling methods like beam search. "},{"title":"Kyle's Market Micro-architecture Model","date":"2024-10-11T10:59:55.000Z","url":"/game-theory/kyle-market-model.html","tags":[["Game-Theory","/tags/Game-Theory/"],["Finance","/tags/Finance/"]],"categories":[["game-theory","/categories/game-theory/"]],"content":"The trading behavior in a market is often backed by asymmetric information, making the market a asymmetric game. Albert S. Kyle proposed a micro-structure formulation in 1985, stating that such market consists of three players: an insider, a random noise trader, and a market maker. His successors also proved that there exist an equilibrium with additional conditions. In this post, we will discuss the formulation of Kyle’s model formulated as an extensive form game and an Optimal-Transport based setting of equilibrium. Basic FormulationThe Initial model proposed by Kyle stated three kinds of traders in the market: Insider (XXX): Who knows the true value of the asset and perform strategies accordingly. Noise Trader (ZZZ): Who trades randomly as a Poisson process or Brownian motion. Market Maker (Y): Who observes the market and forms its own belief of the asset based on the summed order flow of insider and noise trader. Since the trading behavior is relatively noisy, one of the key contribution of Kyle’s model is constructing an information-centric approach to analyze the market movement. The market can be formulated as a game tree with nodes formulated as TV:={τ∈T: τ=r or τ=(I1,i1,x1,z1,…,zt)}TX:={τ∈T: τ=I1,i1 or τ=(I1,i1,x1,z1,…,It,it)}TZ:={τ∈T: τ=(I1,i1,x1) or τ=(I1,i1,x1,z1,…,It,it,xt)} \\begin{align*} &amp;\\mathbb T_V := \\{ \\tau \\in \\mathbb T :\\; \\tau = r \\text{ or } \\tau = (I_{1, i_1}, x_1, z_1, \\ldots, z_t) \\} \\\\ &amp;\\mathbb T_X := \\{ \\tau \\in \\mathbb T :\\; \\tau = I_{1, i_1} \\text{ or } \\tau = (I_{1, i_1}, x_1, z_1, \\ldots, I_{t, i_t})\\} \\\\ &amp;\\mathbb T_Z := \\{ \\tau \\in \\mathbb T :\\; \\tau = (I_{1, i_1}, x_1) \\text{ or } \\tau = (I_{1, i_1}, x_1, z_1, \\ldots, I_{t, i_t}, x_t) \\} \\end{align*} ​TV​:={τ∈T:τ=r or τ=(I1,i1​​,x1​,z1​,…,zt​)}TX​:={τ∈T:τ=I1,i1​​ or τ=(I1,i1​​,x1​,z1​,…,It,it​​)}TZ​:={τ∈T:τ=(I1,i1​​,x1​) or τ=(I1,i1​​,x1​,z1​,…,It,it​​,xt​)}​ Each round of trading can be broken down into two steps. Firstly, new information about the true value of the asset is revealed to the insider. Secondly, both insider and noise trader simultaneously trade a discrete quantity of shares on each price level. EX:={x1,…,xK}⊆REZ:={z1,…,zL}⊆RK,L∈N. E_X := \\left\\{x^1, \\ldots, x^K\\right\\}\\subseteq\\mathbb R \\\\ E_Z := \\left\\{z^1, \\ldots, z^L\\right\\}\\subseteq\\mathbb R \\\\K,L\\in\\mathbb N. EX​:={x1,…,xK}⊆REZ​:={z1,…,zL}⊆RK,L∈N. Information FlowThe insider holds private information about the true value as a distribution. There are NNN fundamental information states and the true value is a mapping v:I→R,i↦vi,I:={1,2,...N} v:I\\rightarrow\\mathbb{R},\\quad i\\mapsto v^i, I:=\\{1,2,...N\\} v:I→R,i↦vi,I:={1,2,...N} The flow of information at time ttt is defined as a set-valued function of III. It={It,1,...It,Nt}⊆2I \\mathcal I_t=\\{I_{t,1},...I_{t,N_t}\\}\\subseteq 2^I It​={It,1​,...It,Nt​​}⊆2I Belief formulation &amp; Pricing System The transition of nodes are formulated by the pricing system of market maker and strategy of insider. The direct successors of game tree τ\\tauτ is given by pV(τ,It+1,it+1):=∑i∈It+1,it+1ν({i})∑j∈It,itν({j})for t∈{1,…,T}, It+1,it+1⊆It,it, \\begin{aligned} p_V (\\tau, I_{t+1, i_{t+1}}) := \\frac{\\sum_{i \\in I_{t+1, i_{t+1}}} \\nu (\\{i\\})}{\\sum_{j \\in I_{t, i_t}} \\nu (\\{j\\})} \\end{aligned} \\qquad \\begin{aligned} &amp; \\text{for } t \\in \\{1, \\ldots, T\\},\\ I_{t+1, i_{t+1}} \\subseteq I_{t, i_{t}},\\\\ &amp; \\end{aligned} pV​(τ,It+1,it+1​​):=∑j∈It,it​​​ν({j})∑i∈It+1,it+1​​​ν({i})​​​for t∈{1,…,T}, It+1,it+1​​⊆It,it​​,​ Equilibrium Setting A Kyle equilibrium is a pair (ξ∗,S∗)∈Ξ×S(\\xi^*, S^*)\\in \\Xi\\times \\mathcal S(ξ∗,S∗)∈Ξ×S satisfying: Given S∗S^*S∗ , the strategy ξ∗\\xi^*ξ∗ is optimal. Given ξ∗\\xi^*ξ∗ , the pricing system S∗S^*S∗ is rational. Optimal Transport SolutionThere are a few works examining nonlinear strategies 𝑋 and the uniqueness of linear strategies in Kyle (1985). Single-Period Kyle ModelCho and El Karoui (2000) find a nonlinear strategy for the single-period Kyle model if they use a Bernoulli distribution for the noise term. For continuous noise (i.e. non-atomic distributions), they also characterize the existence of a unique (linear) equilibrium. Boulatov, Kyle, and Livdan (2012) show the linear strategy is unique for the original single-period Kyle model setup. Boulatov and Bernhardt (2015) also examine a single-period case and show that the linear strategy is unique and robust while nonlinear strategies are not robust. Thus the linear strategy is the equilibrium. Multi-Period Kyle ModelFoster and Viswanathan (1993) show that for multi-period Kyle models, the linear strategy is a unique equilibrium for beliefs in the class of elliptical distributions (e.g. the Gaussian distribution used by Kyle). Continuous-time Kyle ModelBack (1992) shows that in the continuous-time Kyle model, there may be nonlinear strategies. The strategies 𝑋 are, however, smooth and monotone in the total order size. As an interesting aside, Back and Baruch (2004) study conditions where the continuous-time Kyle model converges to the same equilibrium as the Glosten and Milgrom (1985) model. Stochastic Models of Market Microstructure"},{"title":"From MPC to Diffusion Policy","date":"2024-10-05T13:16:21.000Z","url":"/EBM/mpc-diffusion-policy.html","tags":[["robotics","/tags/robotics/"],["diffusion","/tags/diffusion/"],["EBM","/tags/EBM/"]],"categories":[["EBM","/categories/EBM/"]],"content":"In this post, we will try to connect Energy-Based Model with classical optimal control frameworks like Model-Predictive Control from the perspective of Lagrangian optimization. This is one part of the series about energy-based learning and optimal control. A recommended reading order is: Notes on “The Energy-Based Learning Model” by Yann LeCun, 2021 Learning Data Distribution Via Gradient Estimation [From MPC to Energy-Based Policy] How Would Diffusion Model Help Robot Imitation Causality hidden in EBM Review of EKF and MPCConsider a state-space model with process noise and measurement noise as: {x[k]=f(x[k−1],u[k−1],w[k−1]),w∼N(0,Qw)z[k]=h(x[k],v[k]),v∼N(0,Qv)\\left\\{ \\begin{aligned} x[k] &amp;= f(x[k-1], u[k-1], w[k-1]),\\quad &amp;w \\sim \\mathcal{N}(0, Q_w) \\\\\\\\ z[k] &amp;= h(x[k], v[k]),\\quad &amp;v \\sim \\mathcal{N}(0, Q_v) \\end{aligned} \\right.⎩⎨⎧​x[k]z[k]​=f(x[k−1],u[k−1],w[k−1]),=h(x[k],v[k]),​w∼N(0,Qw​)v∼N(0,Qv​)​ With a state xxx and observation zzz. To perform optimal control on such system, we first predict and update our observation with Extended Kalman Filter: Prediction Step Priori Estimation: x^−[k]=f(x^[k−1],u[k−1],0)\\hat{x}^{-}[k] = f(\\hat{x}[k-1], u[k-1], 0)x^−[k]=f(x^[k−1],u[k−1],0) Jacobian of the State Transition Function: A[k]=∂f∂x(x^[k−1],u[k−1],0)A[k] = \\frac{\\partial f}{\\partial x} (\\hat{x}[k-1], u[k-1], 0)A[k]=∂x∂f​(x^[k−1],u[k−1],0) W[k]=∂f∂w(x^[k−1],u[k−1],0)W[k]=\\frac{\\partial f}{\\partial w} (\\hat{x}[k-1], u[k-1], 0)W[k]=∂w∂f​(x^[k−1],u[k−1],0) Error Covariance Prediction: P−[k]=A[k]P[k−1]AT[k−1]+W[k]QwWT[k]P^{-}[k] = A[k]P[k-1]A^T[k-1]+W[k]Q_wW^T[k]P−[k]=A[k]P[k−1]AT[k−1]+W[k]Qw​WT[k] Update Step Jacobian of the Measurement Function: H[k]=∂h∂x(x^−[k],0)H[k] = \\frac{\\partial h}{\\partial x} (\\hat{x}^{-}[k], 0)H[k]=∂x∂h​(x^−[k],0) V[k]=∂h∂v(x^−[k],0)V[k] = \\frac{\\partial h}{\\partial v} (\\hat{x}^{-}[k], 0)V[k]=∂v∂h​(x^−[k],0) Kalman Gain: K[k]=P−[k]H⊤[k]H[k]P−[k]H⊤[k]+V[k]QvVT[k]K[k] = \\frac{P^{-}[k] H^{\\top}[k]} { H[k] P^{-}[k] H^{\\top}[k] + V[k]Q_vV^T[k] }K[k]=H[k]P−[k]H⊤[k]+V[k]Qv​VT[k]P−[k]H⊤[k]​ Posterior Estimation: x^[k]=x^−[k]+K[k](z[k]−h(x^−[k],0))\\hat{x}[k] = \\hat{x}^{-}[k] + K[k] \\left( z[k] - h(\\hat{x}^{-}[k],0) \\right)x^[k]=x^−[k]+K[k](z[k]−h(x^−[k],0)) Error Covariance Update: P[k]=(I−K[k]H[k])P−[k]P[k] = \\left( I - K[k] H[k] \\right) P^{-}[k]P[k]=(I−K[k]H[k])P−[k] With the estimated state we can perform Model-Based Control with the following condition: min⁡uJ=∑i=0N−1ℓ(x[k+i],u[k+i])+ℓf(x[k+N])ℓ(x,u)=(x−xref)⊤Q(x−xref)+u⊤Rus.t.x[k+i+1]=f(x[k+i],u[k+i])∀i=0,…,N−1x[k]=x^[k]xmin≤x[k+i]≤xmax∀iumin≤u[k+i]≤umax∀i\\begin{aligned} \\min_{\\mathbf{u}} \\quad J &amp;= \\sum_{i=0}^{N-1} \\ell(x[k+i], u[k+i]) + \\ell_f(x[k+N]) \\\\\\\\ \\ell(x, u) &amp;= (x - x_{\\text{ref}})^{\\top} Q (x - x_{\\text{ref}}) + u^{\\top} R u \\\\\\\\ \\text{s.t.} \\quad &amp; x[k+i+1] = f(x[k+i], u[k+i]) \\quad \\forall i = 0, \\dots, N-1 \\\\\\\\ &amp; x[k] = \\hat{x}[k] \\\\\\\\ &amp; x_{\\text{min}} \\leq x[k+i] \\leq x_{\\text{max}} \\quad \\forall i \\\\\\\\ &amp; u_{\\text{min}} \\leq u[k+i] \\leq u_{\\text{max}} \\quad \\forall i \\end{aligned}umin​Jℓ(x,u)s.t.​=i=0∑N−1​ℓ(x[k+i],u[k+i])+ℓf​(x[k+N])=(x−xref​)⊤Q(x−xref​)+u⊤Rux[k+i+1]=f(x[k+i],u[k+i])∀i=0,…,N−1x[k]=x^[k]xmin​≤x[k+i]≤xmax​∀iumin​≤u[k+i]≤umax​∀i​ u={u[k],u[k+1],…,u[k+N−1]} \\mathbf{u} = \\{ u[k], u[k+1], \\dots, u[k+N-1] \\} u={u[k],u[k+1],…,u[k+N−1]}: Control input sequence. N N N: Prediction horizon. ℓ(x,u) \\ell(x, u) ℓ(x,u): Stage cost function. ℓf(x) \\ell_f(x) ℓf​(x): Terminal cost function. xmin,xmax x_{\\text{min}}, x_{\\text{max}} xmin​,xmax​: State constraints. umin,umax u_{\\text{min}}, u_{\\text{max}} umin​,umax​: Control constraints. Introducing Energy Based ModelWe can replace the state transition function fff in the formulated state space model with EBM as: p(x[k]∣x[k−1],u[k−1])=e−E(x[k],x[k−1],u[k−1])Z(x[k−1],u[k−1]) p(x[k]∣x[k−1],u[k−1])=\\frac{e^{−E(x[k],x[k−1],u[k−1])}}{Z(x[k−1],u[k−1])} p(x[k]∣x[k−1],u[k−1])=Z(x[k−1],u[k−1])e−E(x[k],x[k−1],u[k−1])​ Supervised TrainingSince ZZZ is often intractable, we will use score matching to learn the EBM in the following content. Here we have the score function as: sθ(x[k],x[k−1],u[k−1])=sθ(x)=∇xlog⁡pθ(x)=−∇xEθ(x) s_\\theta (x[k],x[k−1],u[k−1])=s_\\theta(\\mathbf x) = \\nabla_{\\mathbf x} \\log p_\\theta(\\mathbf x)=-\\nabla_{\\mathbf x} E_\\theta(\\mathbf x) sθ​(x[k],x[k−1],u[k−1])=sθ​(x)=∇x​logpθ​(x)=−∇x​Eθ​(x) Dataset: A set of transitions {(xi[k−1],ui[k−1],xi[k])}i=1N\\{(x_i[k-1], u_i[k-1], x_i[k])\\}^N_{i=1}{(xi​[k−1],ui​[k−1],xi​[k])}i=1N​ with “independently and identically distributed (i.i.d.)” assumption. The training objective is to minimize the difference between data landscape sdata(x)s_{data}(\\mathbf x)sdata​(x) and model landscape sθ(x)s_\\theta(\\mathbf x)sθ​(x), and the objective function is defined as follows, where loss L\\mathcal LL is commonly defined as MSELoss: J(θ)=12Epdata[L(sdata(x),sθ(x))] =12Epdata[∣∣sdata(x)−sθ(x)∣∣2] J(\\theta)=\\frac{1}{2}\\mathbb{E}_{p_{data}}[\\mathcal L(s_{data}(\\mathbf x), s_{\\theta}(\\mathbf x))] \\ =\\frac{1}{2}\\mathbb{E}_{p_{data}}[||s_{data}(\\mathbf x)- s_{\\theta}(\\mathbf x)||^2] J(θ)=21​Epdata​​[L(sdata​(x),sθ​(x))] =21​Epdata​​[∣∣sdata​(x)−sθ​(x)∣∣2] HOWEVER, we cannot get access to full data distribution. According to (Hyvärinen, et.al , 2005)1 we may use the following procedures. (More in Appendix A of original paper) J(θ)=12Epdata[∥sθ(x)∥2−2sθ(x)⊤sdata(x)+∥sdata(x)∥2]=12Epdata[∥sθ(x)∥2]−Epdata[sθ(x)⊤sdata(x)]+12Epdata[∥sdata(x)∥2]⏞constantJ′(θ)=12Epdata[∥sθ(x)∥2]−Epdata[sθ(x)⊤sdata(x)]\\begin{aligned} J(\\theta)&amp;=\\frac{1}{2}\\mathbb{E}_{p_{data}}[\\left\\| s_\\theta(\\mathbf x) \\right\\|^2 - 2 s_\\theta(\\mathbf x)^\\top s_{\\text{data}}(\\mathbf x) + \\left\\| s_{\\text{data}}(\\mathbf x) \\right\\|^2] \\\\ &amp;=\\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}} \\left[ \\left\\| s_\\theta(\\mathbf x) \\right\\|^2 \\right] - \\mathbb{E}_{p_{\\text{data}}} \\left[ s_\\theta(\\mathbf x)^\\top s_{\\text{data}}(\\mathbf x) \\right] + \\overbrace{\\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}} \\left[ \\left\\| s_{\\text{data}}(\\mathbf x) \\right\\|^2 \\right]}^{\\text{constant}} \\\\ J&#x27;(\\theta)&amp;=\\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}} \\left[ \\left\\| s_\\theta(\\mathbf x) \\right\\|^2 \\right] - \\mathbb{E}_{p_{\\text{data}}} \\left[ s_\\theta(\\mathbf x)^\\top s_{\\text{data}}(\\mathbf x) \\right] \\end{aligned}J(θ)J′(θ)​=21​Epdata​​[∥sθ​(x)∥2−2sθ​(x)⊤sdata​(x)+∥sdata​(x)∥2]=21​Epdata​​[∥sθ​(x)∥2]−Epdata​​[sθ​(x)⊤sdata​(x)]+21​Epdata​​[∥sdata​(x)∥2]​constant​=21​Epdata​​[∥sθ​(x)∥2]−Epdata​​[sθ​(x)⊤sdata​(x)]​ Epdata[sθ(x)⊤sdata(x)]=∫pdata(x)sθ(x)⊤sdata(x)dx =∫sθ(x)⊤∇xpdata(x)dx \\mathbb{E}_{p_{\\text{data}}} \\left[ s_\\theta(\\mathbf x)^\\top s_{\\text{data}}(\\mathbf x) \\right] = \\int p_{\\text{data}}(\\mathbf x) s_\\theta(\\mathbf x)^\\top s_{\\text{data}}(\\mathbf x) d\\mathbf x \\ = \\int s_\\theta(\\mathbf x)^\\top \\nabla_{\\mathbf x} p_{\\text{data}}(\\mathbf x) d\\mathbf x Epdata​​[sθ​(x)⊤sdata​(x)]=∫pdata​(x)sθ​(x)⊤sdata​(x)dx =∫sθ​(x)⊤∇x​pdata​(x)dx By integrating by parts, we move the derivative from pdata(x) p_{\\text{data}}(x) pdata​(x) to sθ(x) s_\\theta(x) sθ​(x): ∫sθ(x)⊤∇xpdata(x)dx=−∫pdata(x)divxsθ(x)dxdivxsθ(x)=divx(−∇xEθ(x))=−divx∇xEθ(x)=−ΔxEθ(x)Epdata[sθ(x)⊤sdata(x)]=−Epdata[divxsθ(x)] \\int s_\\theta(\\mathbf x)^\\top \\nabla_{\\mathbf x} p_{\\text{data}}(\\mathbf x) d\\mathbf x = -\\int p_{\\text{data}}(\\mathbf x) \\text{div}_{\\mathbf x} s_\\theta(\\mathbf x) d\\mathbf x \\\\ \\text{div}_{\\mathbf x} s_\\theta(\\mathbf x) = \\text{div}_{\\mathbf x} \\left( -\\nabla_{\\mathbf x} E_\\theta(\\mathbf x) \\right) = -\\text{div}_{\\mathbf x} \\nabla_{\\mathbf x} E_\\theta(\\mathbf x) = -\\Delta_{\\mathbf x} E_\\theta(\\mathbf x)\\\\ \\mathbb{E}_{p_{\\text{data}}} \\left[ s_\\theta(\\mathbf x)^\\top s_{\\text{data}}(\\mathbf x) \\right] = -\\mathbb{E}_{p_{\\text{data}}} \\left[ \\text{div}_{\\mathbf x} s_\\theta(\\mathbf x) \\right] ∫sθ​(x)⊤∇x​pdata​(x)dx=−∫pdata​(x)divx​sθ​(x)dxdivx​sθ​(x)=divx​(−∇x​Eθ​(x))=−divx​∇x​Eθ​(x)=−Δx​Eθ​(x)Epdata​​[sθ​(x)⊤sdata​(x)]=−Epdata​​[divx​sθ​(x)] Eventually we obtain the updated objective function J(θ)=12Epdata[∥∇xEθ(x)∥2]+Epdata[ΔxEθ(x)]Jk(θ)=Tr(∇x[k]2E(x[k]))+12∥∇x[k]E(x[k])∥2 \\begin{aligned} J(\\theta) &amp;= \\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}} \\left[ \\left\\| \\nabla_{\\mathbf x} E_\\theta(\\mathbf x) \\right\\|^2 \\right] + \\mathbb{E}_{p_{\\text{data}}} \\left[ \\Delta_{\\mathbf x} E_\\theta(\\mathbf x) \\right] \\\\ J_k(\\theta) &amp;= \\text{Tr} \\left( \\nabla_{\\mathbf x[k]}^2 E(\\mathbf x[k]) \\right) + \\frac{1}{2} \\left\\| \\nabla_{\\mathbf x[k]} E(\\mathbf x[k]) \\right\\|^2 \\end{aligned} J(θ)Jk​(θ)​=21​Epdata​​[∥∇x​Eθ​(x)∥2]+Epdata​​[Δx​Eθ​(x)]=Tr(∇x[k]2​E(x[k]))+21​​∇x[k]​E(x[k])​2​ Tr(∇x[k]2E(x[k]))\\text{Tr} \\left( \\nabla_{\\mathbf x[k]}^2 E(\\mathbf x[k]) \\right)Tr(∇x[k]2​E(x[k])) denotes the trace of Hessian matrix (or Jacobian) of score function w.r.t. x[k]\\mathbf x[k]x[k] . If you haven’t seen such formulation in diffusion models and feel strange: The training objective is to learn the distribution of q(xt−1∣xt,x0)q(x_{t-1}|x_t, x_0)q(xt−1​∣xt​,x0​), which is a known Gaussian distribution since the noise level is provided. This is also why q-sampling requires x0x_0x0​ . Optimization Based InferencingLangevin Dynamics can produce samples from a probability density p(x)p(\\mathbf x)p(x) using only the score function ∇xlog⁡p(x)\\nabla_{\\mathbf x}\\log p(\\mathbf x)∇x​logp(x). Given a fixed step size ϵ&gt;0\\epsilon &gt;0ϵ&gt;0, and an initial value x~∼π(x)\\tilde{\\mathbf x} \\sim \\pi(\\mathbf x)x~∼π(x) with π\\piπ being a prior distribution, the Langevin method recursively computes the following x~t=x~t−1+ϵ2∇xlog⁡p(x~t−1)+ϵzt \\tilde{\\mathbf x}_t=\\tilde{\\mathbf x}_{t-1}+\\frac{\\epsilon}{2}\\nabla_{\\mathbf x}\\log p(\\tilde{\\mathbf x}_{t-1})+\\sqrt{\\epsilon}\\mathbf z_t x~t​=x~t−1​+2ϵ​∇x​logp(x~t−1​)+ϵ​zt​ where zt∼N(0,I)\\mathbf z_t \\sim \\mathcal N(0,I)zt​∼N(0,I) . The distribution of x~t\\tilde{\\mathbf x}_tx~t​ equals p(x)p(\\mathbf x)p(x) when ϵ→0\\epsilon \\rightarrow 0ϵ→0 and T→∞T \\rightarrow \\inftyT→∞ , or else a Metropolis-Hastings update is needed to correct the error. 1.Hyvärinen, A., &amp; Dayan, P. (2005). Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4). ↩"},{"title":"Notes on 'The Energy-Based Learning Model' by Yann LeCun, 2021","date":"2024-10-01T08:26:45.000Z","url":"/EBM/lecun-ebm-2021.html","tags":[["EBM","/tags/EBM/"],["self-supervised-learning","/tags/self-supervised-learning/"]],"categories":[["EBM","/categories/EBM/"]],"content":"In this note we connect the energy concepts including Lagrangian Dynamics and Gibbs Formula to measuring the quality of prediction. Then we go over the inferencing and training in multi-modal scenarios. This is one part of the series about energy-based learning and optimal control. A recommended reading order is: Notes on “The Energy-Based Learning Model” by Yann LeCun, 2021 Learning Data Distribution Via Gradient Estimation From MPC to Energy-Based Policy How Would Diffusion Model Help Robot Imitation Causality hidden in EBM Recording Link: Yann LeCun | May 18, 2021 | The Energy-Based Learning Model Further Readings: A Tutorial on Energy-Based Learning Reformulation of Back-propagation as Lagrangian OptimizationInstead of forces, Lagrangian mechanics uses energy as a unified parameter. A Lagrangian is a function which summarizes the dynamics of the entire system. The non-relativistic Lagrangian for a system of particles in the absence of an electromagnetic field is given by L=T−V L=T-V L=T−V where TTT denotes total kinetic energy of the system and VVV denotes the total potential energy, reflecting the energy of interaction between the particles. The optimization target is to minimize LLL Lagrange’s Equations for a time varying system with number of constraints being CCC . Particles are labeled as k=1,2,...,Nk=1,2,...,Nk=1,2,...,N and have positions rk≡(xk,yk,zk)r_k\\equiv (x_k,y_k,z_k)rk​≡(xk​,yk​,zk​) and velocity vk≡r˙kv_k \\equiv \\dot r_kvk​≡r˙k​ . ∂L∂rk−ddt∂L∂r˙k+∑i=1Cλi∂fi∂rk=0 \\frac{\\partial L}{\\partial r_k}- \\frac{\\mathrm d}{\\mathrm d t}\\frac{\\partial L}{\\partial \\dot r_k}+ \\sum^{C}_{i=1}\\lambda_i\\frac{\\partial f_i}{\\partial r_k} =0 ∂rk​∂L​−dtd​∂r˙k​∂L​+i=1∑C​λi​∂rk​∂fi​​=0 For each constraint equation fif_ifi​, there’s a Lagrange multiplier λi\\lambda_iλi​ Gibbs Formula [TODO]Gibbs energy was developed in the 1870’s by Josiah Willard Gibbs. He originally termed this energy as the “available energy” in a system. His paper “Graphical Methods in the Thermodynamics of Fluids” published in 1873 outlined how his equation could predict the behavior of systems when they are combined. This quantity is the energy associated with a chemical reaction that can be used to do work, and is the sum of its enthalpy HHH and the product of the temperature TTT and the entropy SSS of the system. G:=H−TS G:=H-TS G:=H−TS Further reading: The Markov blankets of life: autonomy, active inference and the free energy principle Statement in DL Loss L(x,y,w)=C(zK,y)L(x,y,w)=C(z_K,y)L(x,y,w)=C(zK​,y) s.t. zk+1=gk(zk,wk)z_{k+1}=g_k(z_k,w_k)zk+1​=gk​(zk​,wk​), z0=xz_0=xz0​=x Lagrangian for optimization under constraints L(x,y,z,λ,w)=C(zK,y)+∑k=0K−1λkT(zk+1−gk(zk,wk))L(x,y,z,\\lambda,w)=C(z_K,y)+\\sum^{K-1}_{k=0}\\lambda^T_k (z_{k+1}-g_k(z_k,w_k))L(x,y,z,λ,w)=C(zK​,y)+∑k=0K−1​λkT​(zk+1​−gk​(zk​,wk​)) Optimality conditions ∂L(x,y,z,λ,w)∂zk=0,∂L(x,y,z,λ,w)∂λk=0,∂L(x,y,z,λ,w)∂wk=0 \\frac{\\partial L(x,y,z,\\lambda,w)}{\\partial z_k}=0, \\frac{\\partial L(x,y,z,\\lambda,w)}{\\partial \\lambda_k}=0, \\frac{\\partial L(x,y,z,\\lambda,w)}{\\partial w_k}=0 ∂zk​∂L(x,y,z,λ,w)​=0,∂λk​∂L(x,y,z,λ,w)​=0,∂wk​∂L(x,y,z,λ,w)​=0 ∂L(x,y,z,λ,w)∂zk=λk−1T−λkT∂gk−1(zk−1,wk−1)∂zk=0 \\frac{\\partial L(x,y,z,\\lambda,w)}{\\partial z_k}= \\lambda^T_{k-1}-\\lambda^T_k \\frac{\\partial g_{k-1}(z_{k-1},w_{k-1})}{\\partial z_k} =0 ∂zk​∂L(x,y,z,λ,w)​=λk−1T​−λkT​∂zk​∂gk−1​(zk−1​,wk−1​)​=0 In back propagation, the Lagrange multiplier λ\\lambdaλ is the gradient Self Supervised Learning Learning hierarchical representations Learning predictive models Uncertainty&#x2F;multi-modality? Energy Based ModelUsing divergence measure as metrics cannot deal with futures with multiple possibilities (e.g. Multiple solutions in path planning). The average of all possibilities will possibly be the optimal result in such metrics, overfitting may also occur for datasets of insufficient samples. We can replace the divergence measure with an energy function, which measures the “incompatibility” between xxx and yyy. the compatible solution can be inferred using gradient descent, heruistic search etc. Now the target becomes “Finding an output satisfying the constraints”. An unconditional EBM F(y)F(y)F(y) measures the compatibility between components of yyy. For a conditional EBM F(x,y)F(x,y)F(x,y), we have low energy near provided data points, while higher energy for everywhere else. (PS. This optimization is in inference process) y^=arg⁡min⁡yF(x,y) \\hat y = \\arg\\min_y F(x,y) y^​=argymin​F(x,y) As a visualization: Probabilistic models are a special case of EBM. Energies are like un-normalized negative log probabilities 1 . If we want to turn energy functions as distributions, we can use Gibbs-Boltzmann distribution, which adopts maximum entropy approach and Gibbs formula. P(y∣x)=e−βF(x,y)∫y′e−βF(x,y′) P(y|x)=\\frac{e^{-\\beta F(x,y)}}{\\int_{y&#x27;}e^{-\\beta F(x,y&#x27;)}} P(y∣x)=∫y′​e−βF(x,y′)e−βF(x,y)​ β\\betaβ is a positive constant. However the normalization constant at denominator is often intractable. One possible solution is to learn the log-likelihood of P(y∣x)P(y|x)P(y∣x) , and the distribution is changed into ∇ylog⁡P(y∣x)=−βF(x,y)−∇ylog⁡∫y′e−βF(x,y′)⏞≈0 \\nabla_y \\log P(y|x)=-\\beta F(x,y)-\\overbrace{\\nabla_y \\log {\\int_{y&#x27;}e^{-\\beta F(x,y&#x27;)}}}^{\\approx 0} ∇y​logP(y∣x)=−βF(x,y)−∇y​log∫y′​e−βF(x,y′)​≈0​ EBM for Multi-Modal ScenariosJoint Embedding For the above example, the energy function is trained with the similarity of xxx and yyy . There may exist multiple unseen yyy that has the same xxx , and we may quantify those unseen yyy by projecting them into the invariant subspace of latent yyy. Latent Generative EBM Ideally the latent variable represents the independent explanatory factors of variation of prediction. But since it’s unobservable, information capacity of latent variable must be minimized. We may also see it as a “bias”, or a placeholder for uncertainties. The inference can be formulated as: y^,z^=arg⁡min⁡y,zE(x,y,z) \\hat y, \\hat z = \\arg \\min_{y,z} E(x,y,z) y^​,z^=argy,zmin​E(x,y,z) However the latent variable is not presented and cannot be measured in supervised manner. So we will have to minimize its effect. F∞(x,y)=min⁡zE(x,y,z) F_\\infty (x,y) = \\min_z E(x,y,z) F∞​(x,y)=zmin​E(x,y,z) FFF is a free energy model and constrained on \"temperature\" term β\\betaβ as previously shown in Gibbs-Boltzmann distribution. In practice, β\\betaβ can be a variance schedule (eg. DDPM). Fβ=−1βlog⁡∫ze−βE(x,y,z) F_\\beta =-\\frac{1}{\\beta} \\log \\int_z e^{-\\beta E(x,y,z)} Fβ​=−β1​log∫z​e−βE(x,y,z) y^=arg⁡min⁡yF(x,y) \\hat y =\\arg \\min_y F(x,y) y^​=argymin​F(x,y) If we try to understand this in a causal approach, we can treat zzz as a unobserved confounder, which causes a biased estimation of the effect of xxx on EEE. P(E∣do(x))≠P(E∣x) P(E|\\mathrm{do}(x)) \\neq P(E|x) P(E∣do(x))=P(E∣x) This do(⋅)\\mathrm{do}(\\cdot)do(⋅) operator means an intervention (or call it adjustment) of xxx. The intervention cannot “back-propagate” to confounder thus the result is changed. The “adjust formula” provides an unbiased solution by taking all zzz into consideration. However the domain of zzz is often intractable and we need to regularize it so it’s small enough to neglect. P(E∣do(x))=∫zP(E∣x,z)P(z) P(E|\\mathrm{do}(x)) = \\int_z P(E|x,z)P(z) P(E∣do(x))=∫z​P(E∣x,z)P(z) On the contrary, we can also examine the robustness of EEE with sensitivity analysis. This connects to the concept of “quantify the uncertainty of unseen yyy by projecting them into the invariant subspace of latent yyy“ mentioned above. For those may be interested: Sensitivity analysis of such EBM: *For simplicity, we directly write xxx for Pred(x)\\mathrm{Pred}(x)Pred(x) . * If we treat the above generative EBM as a causal graph, assuming linear relationship among variables (non-linear scenes will be derived in the following posts), we will have: &gt;{&gt;&gt;Dec=αxx+αzz&gt;C=βxx+βzz+βyy+δDec&gt;&gt;&gt; &gt; \\left\\{ &gt; \\begin{aligned} &gt; \\mathrm{Dec}&amp;=\\alpha_x x+\\alpha_z z \\\\ &gt; C&amp;=\\beta_x x + \\beta_z z +\\beta_y y+\\delta \\mathrm{Dec} &gt; \\end{aligned} &gt; \\right. &gt; &gt;{&gt;&gt;Dec&gt;C​=αx​x+αz​z=βx​x+βz​z+βy​y+δDec&gt;​&gt;&gt; We can obtain the confounding bias by adjusting xxx, leading to different y^\\hat yy^​, and gets confounding bias: &gt;Bias=αzβz&gt; &gt; \\mathrm{Bias}=\\frac{\\alpha_z}{\\beta_z} &gt; &gt;Bias=βz​αz​​&gt; The Average total Effect (ATE) of zzz on CCC is as &gt;dCdz=∂C∂y∂y∂z+&gt;∂C∂y^∂y^∂z=δ+Bias&gt; &gt; \\frac{\\mathrm d C}{\\mathrm dz} = \\frac{\\partial C}{\\partial {y}} \\frac{\\partial {y}}{\\partial {z}}+ &gt; \\frac{\\partial C}{\\partial {\\hat y}} \\frac{\\partial {\\hat y}}{\\partial {z}} = \\delta+\\mathrm{Bias} &gt; &gt;dzdC​=∂y∂C​∂z∂y​+&gt;∂y^​∂C​∂z∂y^​​=δ+Bias&gt; By assuming the distribution of zzz, we can derive the uncertainty of y^\\hat yy^​ by “propagating”. &gt;Var(y^)=(∂y^∂z)2Var(z)+&gt;(∂y^∂x)2Var(x)+&gt;2∂y^∂x∂y^∂zCov(z,x)&gt; &gt; \\mathrm{Var}(\\hat y) = (\\frac{\\partial {\\hat y}}{\\partial {z}})^2\\mathrm{Var}(z) + &gt; (\\frac{\\partial {\\hat y}}{\\partial {x}})^2\\mathrm{Var}(x) + &gt; 2\\frac{\\partial {\\hat y}}{\\partial {x}} \\frac{\\partial {\\hat y}}{\\partial {z}} \\mathrm{Cov}(z,x) &gt; &gt;Var(y^​)=(∂z∂y^​​)2Var(z)+&gt;(∂x∂y^​​)2Var(x)+&gt;2∂x∂y^​​∂z∂y^​​Cov(z,x)&gt; We can use the Fisher Information, which is defined to be the variance of the score function, to further state the uncertainty. This will come in the next post that provides a more detailed formulation of score function and EBM. Training of EBMShape F(x,y)F(x,y)F(x,y) so that: F(x[i],y[i])F(x[i], y[i])F(x[i],y[i]) is strictly smaller than F(x[i],y)F(x[i], y)F(x[i],y) for all yyy different from y[i]y[i]y[i]. Keep FFF smooth. (Max-likelihood probabilistic methods breaks this) More Existing approaches: Contrastive-based L(x,y,y^)=[Fθ(x,y)−Fθ(x,y^)+m(y,y^)]+ L(x,y,\\hat y) = [F_\\theta(x,y) - F_\\theta(x,\\hat y)+m(y,\\hat y)]^+ L(x,y,y^​)=[Fθ​(x,y)−Fθ​(x,y^​)+m(y,y^​)]+ where y^\\hat yy^​ is negative sample. Regularized &#x2F; Architectural methods: Minimize the volume of low-energy regions. ​ e.g. Limit the capacity of latent: L(x,y)=Fθ(x,y)=min⁡z[C(Dec(Pred(x),z),y)+R(z)] L(x,y)=F_\\theta(x,y)=\\min_z [C(\\mathrm{Dec}(\\mathrm{Pred}(x),z),y)+R(z)] L(x,y)=Fθ​(x,y)=zmin​[C(Dec(Pred(x),z),y)+R(z)] AppendixWhy Max-Likelihood Sucks in Contrastive Method Makes the energy landscape into a valley. 1.Further readings of learning data distributions: Generative Modeling by Estimating Gradients of the Data Distribution↩2.KL divergences are comparisons between Gaussians, so they can be calculated in a Rao-Blackwellized fashion with closed form expressions instead of high variance Monte Carlo estimate↩"},{"title":"How Would Diffusion Model Help Robot Imitation","date":"2024-09-01T15:56:13.000Z","url":"/robotics/diffusion-robot-imitation.html","tags":[["robotics","/tags/robotics/"],["diffusion","/tags/diffusion/"],["EBM","/tags/EBM/"]],"categories":[["robotics","/categories/robotics/"]],"content":"In short, using diffusion process instead of directly applying MSE loss on trajectories enables a wider variety of trajectory solutions learnt from imitation data, instead of the trajectory provided by dataset(s), which is beneficial for small-set imitation learning. In this blog post, we will discuss how and why diffusion process can achieve such and trajectory-agnostic result. This is one part of the series about energy-based learning and optimal control. A recommended reading order is: Notes on “The Energy-Based Learning Model” by Yann LeCun, 2021 Learning Data Distribution Via Gradient Estimation From MPC to Energy-Based Policy How Would Diffusion Model Help Robot Imitation Causality hidden in EBM Diffusion Process Revisiting We can start from the target functions of DDPM. The forward process, where noise are added to samples, is formulated as q(xt∣x0)=N(xt;aˉtx0,(1−aˉt)I) q(x_t|x_0)=\\mathcal N(x_t; \\sqrt{\\bar a_t}x_0, (1-\\bar a_t)\\mathbf I) q(xt​∣x0​)=N(xt​;aˉt​​x0​,(1−aˉt​)I) where at=1−βta_t=1-\\beta_tat​=1−βt​, aˉt=∏s=1tas\\bar a_t = \\prod_{s=1}^t a_saˉt​=∏s=1t​as​ schedules the noise level. This can be inferred from q(xt∣xt−1)q(x_t|x_{t-1})q(xt​∣xt−1​) based on its markov chain property. The optimization target of network θ\\thetaθ is formulated as predicting the noise added in the forward process at different noise levels. For example, KL(ϵk,ϵθ(x0+ϵk,k))\\mathrm {KL}(\\epsilon^k, \\epsilon_\\theta(x^0+\\epsilon^k, k))KL(ϵk,ϵθ​(x0+ϵk,k)) , where ϵk\\epsilon^kϵk denotes noise generated with noise level kkk. And if we want to add condition for the model, we can simply use these conditions with cross attention on noise predictors. Paradigm: Learning Distribution by Estimating GradientsIt’s easy for researchers to come out the idea that the conditional observation-action distribution P(At∣Ot)P(A_t | O_t)P(At​∣Ot​) can also be learned by estimation gradients of data distribution, which is a common paradigm of RL and IL. Implicit Policy LearningScore-Based Generative Modeling through Stochastic Differential Equations states that, for score matching, it’s reversing a “variance exploding” SDE where more and more noise is added to data, while diffusion models are reversing a “variance preserving” SDE that interpolates between the data and a fixed variance gaussian. Stochastic Sampling and Initialization Intuitively, multi-modality in action generation for diffusion policy arises from two sources – an underlying stochastic sampling procedure and a stochastic initialization. Trajectory Generation with Diffusion ModelsPeople have been dreaming of using video or scene generation model as “world model” to guide robot planning. However, most of the information in a video frame, even for those captured in real scenarios are highly redundant for the oriented task. In need of a unified representation of states and actions among different training sets, especially visual training sets like Internet videos, trajectory, as a time-dependent elegant modal, is adopted by multiple works, its generation with diffusion is also a low-hanging fruit. Flow as the Cross-domain Manipulation Interface  "},{"title":"Notes on ScoreGrad","date":"2024-08-07T23:22:01.000Z","url":"/diffusion/scoregrad.html","tags":[["diffusion","/tags/diffusion/"],["time series","/tags/time-series/"]],"categories":[["diffusion","/categories/diffusion/"]],"content":"ScoreGrad is a EBM make use of iterative conditional SDE sampling via diffusion to perform multi-variate probabilistic time series prediction. Basically it uses RNN&#x2F;LSTM&#x2F;GRU to encode past time series as a condition and sample a probability distribution of predicting time series based on this. Paper link:  Code release:  ArchitectureConditionerAs a conditional generation task, it use an RNN&#x2F;LSTM and extracts last hidden state as feature. For a time Score Function"},{"title":"Swizzling: The Snake Trying Not to Bite Itself","date":"2024-04-25T00:52:57.000Z","url":"/GPU/swizzling.html","tags":[["Architecture","/tags/Architecture/"],["SIMT","/tags/SIMT/"]],"categories":[["GPU","/categories/GPU/"]],"content":"Abstract To avoid shared memory conflict in shared memory for each thread in different banks, swizzling is invented to create offsets for memory allocation. Which means, finding places for the memory that requested by a Layout. In computer graphics, swizzles are a class of operations that transform vectors by rearranging components. Swizzles can also project from a vector of one dimensionality to a vector of another dimensionality, such as taking a three-dimensional vector and creating a two-dimensional or five-dimensional vector using components from the original vector. Wikipedia, “Swizzling” Previously: Hierarchical Tensor and the Story of Indexing Reference Link: cutlass Motivation Each thread are having their own memory. However, in tasks that needs the joint forces of threads, a shared memory is needed to perform better cooperation. For example, for the tasks of GEMM like D=AB+CD=AB+CD=AB+C, matrices are staying the shared memory waiting to be visited and calculated. Each part of the shared memory is carefully allocated in each bank. Everything looks well, isn’t it? What if they don’t? Let’s first have a glance at how threads extract bits from RAM. To access data, threads provide memory addresses they want, memory controllers will map them into chip selection signal, bank selection signal and physical address on chip(Row and column). Building the mapping of addr-&gt;(bank, physical address). Moreover, the each bank and address index can only be used once. Thus, when it comes to situation where threads are trying to access different address of the same bank, it will cause a “BANK CONFLICT” which makes the data retrieval cannot be done in a single clock cycle. To accelerate the process, people have develop a way of mapping that can maximize the data extracted in each clock cycle. This is the “swizzling” we are talking about today. To simplify the entire process, we will state the physical address as row and banks as column. So the entire memory model looks like a big rectangle. MethodLet me try to revoke your memory about Data structure lectures. You may have heard about “Circular Queue”, and the core of it is taking the remainder as index of head and tail. This is also the core spirit of swizzling. A The swizzling problem can be modeled as follows. Given the memory offset from Layout, which can be seen as the logical address, the Swizzle method is required to find out the physical address on the chip. Offset=Swizzle(Layout(index)) Offset=\\text{Swizzle}(\\text{Layout}(index)) Offset=Swizzle(Layout(index)) Based on the circular queue example, we can easily formulate that we can build a “circular queue” for each thread in which heads and tails belongs to each thread do not show up in the same bank. So we can build a class like this, which can be called by Swizzle(Layout(index)): In one word, there will be 2B2^B2B threads looking for ⌈Layout.size()/2M⌉\\lceil Layout.size()/2^M\\rceil⌈Layout.size()/2M⌉ blocks stored in 2S2^S2S banks. Inside each bank, spaces for ⌈2S/2B⌉\\lceil 2^S/2^B \\rceil⌈2S/2B⌉ blocks are allocated. Inside the space, there will be ⌈(Layout.size()/2M)/2B⌉\\lceil (Layout.size()/2^M)/2^B\\rceil⌈(Layout.size()/2M)/2B⌉ blocks. The maximum “width” of threads shall be 2B/2M2^B/2^M2B/2M, and the maximum banks involved shall be size/2M=2B+Ssize/2^M=2^{B+S}size/2M=2B+S. Let’s first consider the situation where we can query all data we need in one cycle, which means every bank only contains just 1 or 0 “base” block. Since no banks will show up for the second time, we can simplify it as the circular queue above. Inside the big “queue”, all banks are allocated equal size of “sub-queues”, whose head address (Remember the unit is index of Layout) can be: The size is 2B+M+∣S∣2^{B+M+|S|}2B+M+∣S∣ in the unit of Layout index. Wait…How can we get the rear here? Clearly the rear is similar to offset, and it’s related to the pointers that steps towards after each insertion. We only need to convert it to the index in the unit of Layout index. Looks easy, isn’t it? However, these operations are a bit slow in calculation, so we can setup them as shifting and bitwise operations. Calculation to bitwise operations num∣2n=num&gt;&gt;nnum(mod 2n)=num&amp;(2n−1)num⋅2n=num&lt;&lt;n \\begin{aligned} num \\mid 2^n&amp;=num&gt;&gt;n\\\\\\\\ num(\\mod 2^n)&amp;= num \\&amp; (2^n-1) \\\\\\\\ num\\cdot 2^n&amp;=num&lt;&lt;n \\end{aligned} num∣2nnum(mod2n)num⋅2n​=num&gt;&gt;n=num&amp;(2n−1)=num&lt;&lt;n​ Here the 2n−12^n-12n−1 is used as a “mask” to obtain the remainder. Actually, we can simply send the offsets to where they need to go and do calculations with the corresponding slices, and ignore other digits! How can we send them there? Where is the corresponding slices? First, let’s rethink the sub-queue example. An offset can be hashed into a (bnk, thd, blk). Among these variables, who can appear for the second time for another offset without causing bank conflict? The answer is bank! (Hint, think of the stages where banks are not enough and we need to visit RAM for the second time) Let’s take a 32bit logical address as example: p.s. You can read more about XOR and its applications by learning about Cyclic Redundancy Check. Conclusionwhat a marvelous journey back to discrete mathematics, computer architecture and Verilog! This implementation has provided me with a tremendous insight into the application of number theory in designing memory mapping. Previously what I am trying to do is proving everything is alright via strict clock constraints and complex inter-clock domain techniques so it can pass the STA and other test cases. However, via number theory, we can reduce the stress by proving things will work mathematically. Quite a pleasure to be able to sit still and read through the project!"},{"title":"[Reading]PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects","date":"2023-10-01T21:18:03.000Z","url":"/NeRF/PARIS.html","tags":[["NeRF","/tags/NeRF/"],["Articulation Reconstruction","/tags/Articulation-Reconstruction/"]],"categories":[["NeRF","/categories/NeRF/"]],"content":"ABSTRACT: This paper introduces a self-supervised, end-to-end architecture that learns part-level implicit shape and appearance models and optimizes motion parameters jointly without requiring any 3D supervision, motion, or semantic annotation. The training process is similar to original NeRF but and extend the ray marching and volumetric rendering procedure to compose the two fields. [Arxiv] [Github] [Project Page] Problem StatementThe problem of articulate object reconstruction in this paper can be summarized as: Given start state t=0t=0t=0 and end state t=1t=1t=1 and corresponding multi-view RGB images ItI^tIt and camera parameters. The first problem is to decouple the object into static and movable part. Here the paper assumes that an object has only one static and one movable part. The second problem is to estimate the articulated motion T∈{fp,q,fa,d}T\\in\\{f_{p,q}, f_{a,d}\\}T∈{fp,q​,fa,d​} . A revolute joint is parametrize as fp,qf_{p,q}fp,q​ a pivot point p∈R3p\\in\\mathbb R^3p∈R3 and a rotation as quaternion q∈R4q\\in\\mathbb R^4q∈R4, ∣∣q∣∣=1||q||=1∣∣q∣∣=1. A prismatic joint is modeled as fa,df_{a,d}fa,d​ a joint axis as unit vector a∈R3a \\in\\mathbb R^3a∈R3 and a translation distance ddd. The training process will adapt one of them as prior info states. If no such prior info is given, the motion is modeled by SE(3)\\mathbf{SE}(3)SE(3). Method This paper divides the parts by registration on input state ttt to a canonical state t∗t^*t∗. The components agrees with the transformation is extracted as moving part and the remaining as the static part. Structure Static and moving part are jointly learnt during training and they are built separately on networks with the same structure that built upon InstantNGP. Their relationship is modeled explicitly as the transformation function TTT as described in Problem Statement. The fields are represented as: {Static:FS(xt,dt)=σS(xt),cS(xt,dt)Mobile:FS(xt∗,dt∗)=σS(xt∗),cS(xt∗,dt∗) \\left\\{ \\begin{aligned} \\mathtt{Static}:&amp; \\mathcal{F}^S(\\mathbb x_t,\\mathbb d_t)= \\sigma^S(\\mathbb x_t),c^S(\\mathbb x_t,\\mathbb d_t)\\\\ \\mathtt{Mobile}:&amp; \\mathcal{F}^S(\\mathbb x_{t^*},\\mathbb d_{t^*})= \\sigma^S(\\mathbb x_{t^*}),c^S(\\mathbb x_{t^*},\\mathbb d_{t^*})\\\\ \\end{aligned} \\right. {Static:Mobile:​FS(xt​,dt​)=σS(xt​),cS(xt​,dt​)FS(xt∗​,dt∗​)=σS(xt∗​),cS(xt∗​,dt∗​)​ Here xt∈R3\\mathbb x_t\\in \\mathbb R^3xt​∈R3 is a point sampled along a ray at state ttt with direction. dt∈R3\\mathbb d_t\\in \\mathbb R^3dt​∈R3. σ(x)∈R\\sigma(\\mathbb x)\\in \\mathbb Rσ(x)∈R is the density value of the point x, and c(x,d)c(\\mathbb x,\\mathbb d)c(x,d) is the RGB color predicted from the point x from a view direction ddd . TrainingThe adapted training pipeline is similar to NeRF and the ray marching and volumetric rendering procedure to compose the two fields is extended. "},{"title":"【Reading】Ditto-Building Digital Twins of Articulated Objects from Interaction","date":"2023-06-13T16:36:17.000Z","url":"/Interactive-Perception/ditto.html","tags":[["NeRF","/tags/NeRF/"],["Articulation Reconstruction","/tags/Articulation-Reconstruction/"],["Interactive-Perception","/tags/Interactive-Perception/"]],"categories":[["Interactive-Perception","/categories/Interactive-Perception/"]],"content":"This paper propose a way to form articulation model of articulated objects by encoding the features and find the correspondence of static and mobile part via visual observation before and after the interaction. Workflow-In BriefTwo Stream Encoder Given point cloud observations before and after interaction: P1,P2∈RN×3P_1,P_2 \\in \\mathbb R^{N\\times 3}P1​,P2​∈RN×3 Encode them with PointNet++ Encoder μenc\\mu_{enc}μenc​: f1=μenc(P1)f_1=\\mu_{enc}(P_1)f1​=μenc​(P1​), f2=μenc(P2)f_2=\\mu_{enc}(P_2)f2​=μenc​(P2​). f1,f2∈RN′×dsubf_1,f_2\\in \\mathbb R^{N&#x27;\\times d_{sub}}f1​,f2​∈RN′×dsub​. N′&lt;NN&#x27;&lt;NN′&lt;N is the number of the sub-sampled points, and dsubd_{sub}dsub​ is the dimension of the sub-sampled point features. Fuse the features with attention layer: Attn12=softmax(f1f2Tdsub)f2Attn_{12}=softmax(\\frac{f_1f_2^T}{\\sqrt{d_{sub}}})f_2Attn12​=softmax(dsub​​f1​f2T​​)f2​, f12=[f1,Attn12]f_{12}=[f_1,Attn_{12}]f12​=[f1​,Attn12​], f12∈RN′×2dsubf_{12}\\in \\mathbb R^{N&#x27;\\times 2 d_{sub}}f12​∈RN′×2dsub​. The fused feature is decoded by two PointNet++ decoder νgeo\\nu_{geo}νgeo​, νart\\nu_{art}νart​, and get fgeo=νgeo(f12)f_{geo}=\\nu_{geo}(f_{12})fgeo​=νgeo​(f12​), fart=νart(f12)f_{art}=\\nu_{art}(f_{12})fart​=νart​(f12​). f1,f2∈RN×ddensef_1,f_2\\in \\mathbb R^{N\\times{d_{dense}}}f1​,f2​∈RN×ddense​ are point features aligned with P1P_1P1​ Feature encoding based on ConvONet. fartf_{art}fart​ is projected into 2D feature planes and fgeof_{geo}fgeo​ is projected into voxel grids as in the ConvONets. The points that fall into the same pixel cell or voxel cell are aggregated together via max pooling. Training Revolute joint: "},{"title":"【Reading】Ditto in the House-Building Articulation Models of Indoor Scenes through Interactive Perception","date":"2023-06-08T19:41:29.000Z","url":"/Interactive-Perception/ditto-in-the-house.html","tags":[["NeRF","/tags/NeRF/"],["robotics","/tags/robotics/"],["Articulation Reconstruction","/tags/Articulation-Reconstruction/"],["Interactive-Perception","/tags/Interactive-Perception/"]],"categories":[["Interactive-Perception","/categories/Interactive-Perception/"]],"content":"The paper proposed a way of modeling interactive objects in a large-scale 3D space by making affordance predictions and inferring the articulation properties from the visual observations before and after the self-driven interaction. Workflow-In brief Given Initial scene observation Ps∈RNs×6P_s \\in \\mathbb {R}^{N_s \\times 6}Ps​∈RNs​×6, each point is a 6D vector (R,G,B,x,y,z)(R,G,B,x,y,z)(R,G,B,x,y,z) . Get an affordance map As∈RNsA_s\\in \\mathbb {R}^{N_s}As​∈RNs​ and samples peak locations as interaction hotspots Hs∈RNh×3H_s \\in \\mathbb {R}^{N_h\\times 3}Hs​∈RNh​×3 (The locations where the robot can successfully manipulate the articulated objects, and infer the articulation model) from observation PsP_sPs​. For each interaction, robot applies force to the hotspot to produce potential articulated motions. Sample point cloud P∈RNo×3P \\in \\mathbb {R}^{N_o\\times 3}P∈RNo​×3 , P′∈RNo×3P&#x27; \\in \\mathbb {R}^{N_o\\times 3}P′∈RNo​×3 center on the interaction hotspot before and after the interaction. Record contact location c,c′∈R3c,c&#x27; \\in \\mathbb {R}^3c,c′∈R3. An articulation inference network segments the point cloud into static and mobile parts based on P,P′,c,c′P,P&#x27;,c,c&#x27;P,P′,c,c′ . Estimate articulation parameters. [1] Prismatic joint: {up,sp}\\{u^p,s^p\\}{up,sp}, where up∈R3u^p \\in \\mathbb {R}^3up∈R3 is the translation axis, sp∈Rs^p\\in \\mathbb {R}sp∈R is the joint state, which is the relative translation distance. Revolute joint: {ur,q,sr}\\{u^r,q,s^r\\}{ur,q,sr}, where ur∈R3u^r\\in \\mathbb {R}^3ur∈R3 is the revolute axis, q∈R3q\\in \\mathbb {R}^3q∈R3 is the pivot point on the revolute axis, sr∈Rs^r \\in \\mathbb {R}sr∈R is the joint state, which is the relative rotation angle. Map the estimated articulation model of each object from the global frame. The set of these articulation models constitutes the scene-level articulation model MMM. Modules-In detailAffordance Prediction Estimation based on PointNet++[2] The prediction problem is modeled as a binary classification problem. The observation point cloud is fed into PointNet++ to get a point-wise affordance map. Exploration driven training: We first uniformly sample locations over the surface of both articulated and non-articulated parts, then have the robot interact with them. If the robot successfully moves any articulated part of theobjects, we label the corresponding location as positive affordances or negative otherwise. To simulate gripper-based interactions, we perform a collision check at each location to ensure enough space for placing a gripper. The virtual robot interacts with the object as in reality (Pull, push or rotate). For each successful interaction, we collect the robot’s egocentric observations before and after interaction and the object’s articulation model as training supervision. The data distribution is imbalanced due to the large proportion of negative data. To mitigate the imbalance problem, we optimize the network with the combination of the cross-entropy loss and the dice loss。 Non-maximum suppression (NMS) for peak selection select the point with the maximum score and add it to the preserving set. Suppress its neighbors by a certain distance threshold. Repeat this process until all points are added to the preserving set or suppressed. Articulation Inference Ditto: Building Digital Twins of Articulated Objects [3] The occupancy decoder is discarded. RefinementThe estimated articulation model could have a higher accuracy if the observations covered significant articulation motions and a complete view of the object’s interior. However, theseobservations may be partially occluded due to ineffective actions. eg. we find that articulation estimation of a fully opened revolute joint, like &gt;30∘&gt;30^\\circ&gt;30∘, is more accurate in terms of angle error than one with an ajar joint. Accordingly, we develop an iterative procedure of interacting with partially opened joints and refining the articulated predictions. we exploit the potential motion information from the previous articulation model and extract the object-level affordance. We refine the affordance prediction by selecting a pair of locations and actions to produce the most significant articulation motion. We set the force direction as the moment of the axis. Given the joint axis and part segment, we select the point in the predicted mobile part farthest from the joint axis as our next interaction hotspot. References[1] Li, X., Wang, H., Yi, L., Guibas, L. J., Abbott, A. L., &amp; Song, S. (2020). Category-level articulated object pose estimation. In Proceedings of the IEEE&#x2F;CVF Conference on Computer Vision and Pattern Recognition (pp. 3706-3715).  [2] Qi, C. R., Yi, L., Su, H., &amp; Guibas, L. J. (2017). Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems, 30.  pointnet arxiv.org&#x2F;pdf&#x2F;1612.00593.pdf [3] "},{"title":"【Reading】LATITUDE:Robotic Global Localization with Truncated Dynamic Low-pass Filter in City-scale NeRF","date":"2023-04-19T19:22:28.000Z","url":"/NeRF/LATITUDE.html","tags":[["NeRF","/tags/NeRF/"],["robotics","/tags/robotics/"],["UAV","/tags/UAV/"],["localization","/tags/localization/"],["optimization","/tags/optimization/"]],"categories":[["NeRF","/categories/NeRF/"]],"content":"This paper proposes a two-stage localization mechanism in city-scale NeRF. AbstractNeural Radiance Fields (NeRFs) have made great success in representing complex 3D scenes with high-resolution details and efficient memory. Nevertheless, current NeRF-based pose estimators have no initial pose prediction and are prone to local optima during optimization. In this paper, we present LATITUDE: Global Localization with Truncated Dynamic Low-pass Filter, which introduces a two-stage localization mechanism in city-scale NeRF. In place recognition stage, we train a regressor through images generated from trained NeRFs, which provides an initial value for global localization. In pose optimization stage, we minimize the residual between the observed image and rendered image by directly optimizingthe pose on the tangent plane. To avoid falling into local optimum, we introduce a Truncated Dynamic Low-pass Filter (TDLF) for coarse-to-fine pose registration. We evaluate our method on both synthetic and real-world data and show its potential applications for high-precision navigation in large scale city scenes. System Design Place Recognition Original poses, accompanied by additional poses around the original ones are sampled. The pose vector is passed through the trained and fixed Mega-NeRF with shuffled appearance embeddings. Initial poses of the inputted images are predicted by a pose regressor network. Pose Optimization The initial poses are passed through positional encoding filter The pose vector is passed through the trained and fixed Mega-NeRF and generates a rendered image. Calculate the photometric error of the rendered image and the observed image and back propagate to get a more accurate pose with the TDLF. ImplementationPlace Recognition Data Augmentation: A technique in machine learning used to reduce overfitting when training a machine learning model by training models on several slightly-modified copies of existing data. First uniformly sample several positions in a horizontal H×WH\\times WH×W rectangle area around each position around original poses Treal(xreal,qreal)T_{real}(\\mathbf x_{real},\\mathbf q_{real})Treal​(xreal​,qreal​). Then add random perturbations on each axis drawn evenly in [−θ,θ][-\\theta,\\theta][−θ,θ], where θ\\thetaθ is the max amplitude of perturbation to form sampled poses Tsyn(xsyn,qsyn)T_{syn}(\\mathbf x_{syn},\\mathbf q_{syn})Tsyn​(xsyn​,qsyn​). They are used to generate the rendered observations IsynI_{syn}Isyn​ by inputting the poses to Mega-NeRF. To avoid memory explosion, we generate the poses using the method above and use Mega-NeRF to render images during specific epochs of pose regression training. Additionally, Mega-NeRF’s appearance embeddings are selected by randomly interpolating those of the training set, which can be considered as a data augmentation technique to improve the robustness of the APR model under different lighting conditions. Pose Regressor: Absolute pose regressor (APR) networks are trained to estimate the pose of the camera given a captured image. Architecture: Built on top of VGG16’s light network structure, we use 4 full connection layers to learn pose information from image sequences. Input: Observed image IrealI_{real}Ireal​ (resolution 480×240480 \\times 240480×240), rendered observations IsynI_{syn}Isyn​ Output: Corresponding estimated poses T^real(x^real,q^real)\\hat T_{real}(\\mathbf {\\hat x}_{real},\\mathbf {\\hat q}_{real})T^real​(x^real​,q^​real​), T^syn(x^syn,q^syn)\\hat T_{syn}(\\mathbf {\\hat x}_{syn},\\mathbf {\\hat q}_{syn})T^syn​(x^syn​,q^​syn​). Loss Function: (In general, the model should trust more on real-world data and learn more from it.) Lsyn=∥x^syn−xsyn∥2+γ∥q^syn−qsyn∣∣q^syn∣∣∥2Lreal=∥x^real−xreal∥2+γ∥q^real−qreal∣∣q^real∣∣∥2L=Lreal+βLsyn \\begin{aligned} L_{syn}&amp;=\\Vert\\mathbf {\\hat x}_{syn}-\\mathbf {x}_{syn}\\Vert_2+\\gamma\\left\\Vert\\mathbf {\\hat q}_{syn}-\\frac{\\mathbf {q}_{syn}}{||\\mathbf {\\hat q}_{syn}||}\\right\\Vert_2\\\\\\\\ L_{real}&amp;=\\Vert\\mathbf {\\hat x}_{real}-\\mathbf {x}_{real}\\Vert_2+\\gamma\\left\\Vert\\mathbf {\\hat q}_{real}-\\frac{\\mathbf {q}_{real}}{||\\mathbf {\\hat q}_{real}||}\\right\\Vert_2\\\\\\\\ L&amp;=L_{real}+\\beta L_{syn} \\end {aligned} Lsyn​Lreal​L​=∥x^syn​−xsyn​∥2​+γ​q^​syn​−∣∣q^​syn​∣∣qsyn​​​2​=∥x^real​−xreal​∥2​+γ​q^​real​−∣∣q^​real​∣∣qreal​​​2​=Lreal​+βLsyn​​ Pose Optimization MAP Estimation Problem[A] Formulation: P(T^k∣F(ϵ),Ik)∝P(I^k∣T^k,F(ϵ))P(T^k∣Ik) P(\\hat T_k |F(\\epsilon), I_k)\\propto P(\\hat I_k|\\hat T_k, F(\\epsilon))P(\\hat T_k|I_k) P(T^k​∣F(ϵ),Ik​)∝P(I^k​∣T^k​,F(ϵ))P(T^k​∣Ik​) Here P(T^k∣Ik)P(\\hat T_k|I_k)P(T^k​∣Ik​) denotes place recognition; F(ϵ)F(\\epsilon)F(ϵ) denotes the trained Mega-NeRF. We optimize posterior P(T^k∣F(ϵ),Ik)P(\\hat T_k |F(\\epsilon), I_k) P(T^k​∣F(ϵ),Ik​)by minimizing the photometric error of IkI_kIk​ and the image I^k\\hat I_kI^k​ rendered by F(ϵ)F(\\epsilon)F(ϵ). Optimization on Tangent Plane: We optimize pose on tangent plane to ensure a smoother convergence. [1] TODO I know nothing about SE(3)SE(3)SE(3) :( Explanations &amp; References[1]Adamkiewicz, M., Chen, T., Caccavale, A., Gardner, R., Culbertson, P., Bohg, J., &amp; Schwager, M. (2022). Vision-only robot navigation in a neural radiance world. IEEE Robotics and Automation Letters, 7(2), 4606-4613.  Turki, H., Ramanan, D., &amp; Satyanarayanan, M. (2022). Mega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs. In Proceedings of the IEEE&#x2F;CVF Conference on Computer Vision and Pattern Recognition (pp. 12922-12931).  Yen-Chen, L., Florence, P., Barron, J. T., Rodriguez, A., Isola, P., &amp; Lin, T. Y. (2021, September). inerf: Inverting neural radiance fields for pose estimation. In 2021 IEEE&#x2F;RSJ International Conference on Intelligent Robots and Systems (IROS) (pp. 1323-1330). IEEE.  [A]Maximum A Posterior (MAP) Estimation: Maximum a posteriori (MAP) estimation is a method of statistical inference that uses Bayes’ theorem to find the most likely estimate of a parameter given some observed data. Maximum a posteriori estimation - Wikipedia "},{"title":"Reading:\"NeRF:Representing Scenes as Neural Radiance Fields for View Synthesis\"","date":"2023-04-17T19:21:56.000Z","url":"/NeRF/NeRF-startup.html","tags":[["NeRF","/tags/NeRF/"],["papers","/tags/papers/"],["Computer-Vision","/tags/Computer-Vision/"],["Deep-Learning","/tags/Deep-Learning/"]],"categories":[["NeRF","/categories/NeRF/"]],"content":"This is a summary for paper “NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis”. Keywords: scene representation, view synthesis, image-based rendering, volume rendering, 3D deep learning A brief understanding: How to train a network for NeRFTraining a neural network for NeRF (Neural Radiance Fields) involves several steps, including data preparation, network architecture design, training, and evaluation. Data preparation: The first step is to prepare the data that will be used to train the neural network. This typically involves capturing a set of 3D scans of the object or environment being represented, and labeling the data with the corresponding colors that should be associated with each point in the 3D space. Network architecture design: The next step is to design the architecture of the neural network that will be used to represent the object or environment. This typically involves defining the number and types of layers in the network, as well as the size and shape of the network. Training: Once the network architecture has been designed, the next step is to train the network using the prepared data. This involves feeding the data into the network and adjusting the weights of the network over multiple iterations, or epochs, to optimize the performance of the network. Evaluation: After the network has been trained, it is typically evaluated on a separate set of data to measure its performance and ensure that it is generating accurate results. This can involve comparing the output of the network to the ground truth data, as well as using visualization techniques to compare the rendered images produced by the network to actual photographs of the object or environment. Overall, the process of training a neural network for NeRF involves a combination of data preparation, network architecture design, training, and evaluation to produce a highly accurate and efficient 3D representation of an object or environment. ​ By Vicuna-13b Contribution An approach for representing continuous scenes with complex geometry and materials as 5D neural radiance fields, parameterized as basic MLP networks. A differentiable rendering procedure based on classical volume rendering techniques, which we use to optimize these representations from standard RGB images. This includes a hierarchical sampling strategy to allocate the MLP’s capacity towards space with visible scene content. A positional encoding to map each input 5D coordinate into a higher dimensional space, which enables us to successfully optimize neural radiance fields to represent high-frequency scene content. An overview of our neural radiance field scene representation and differentiable rendering procedure. Here g.t. represents the “ground truth”, which means the real scene. Overview of the Rendering Process March camera rays through the scene to generate a sampled set of 3D points. Use those points and their corresponding 2D viewing directions as input to the neural network to produce an output set of colors and densities. use classical volume rendering techniques to accumulate those colors and densities into a 2D image. we can use gradient descent to optimize this model by minimizing the error between each observed image and the corresponding views rendered from our representation. Neural Radiance Field Scene RepresentationThis is a method for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network FΘF_\\ThetaFΘ​, whose input is a single continuous 5D coordinate and whose output is the volume density and view-dependent emitted radiance at that spatial location. FΘ:(x,d)→(c,σ) F_\\Theta: (\\textbf x,\\textbf d)\\rightarrow (\\textbf c,\\sigma) FΘ​:(x,d)→(c,σ) x\\mathbf{x}x: 3D location (x,y,z)(x,y,z)(x,y,z) d\\mathbf dd: 2D viewing direction (θ,ϕ)(\\theta,\\phi)(θ,ϕ) c\\mathbf cc: Emitted color (r,g,b)(r,g,b)(r,g,b) σ\\sigmaσ: Volume density From Object to Scene: Volume Rendering with Radiance FieldsOur 5D neural radiance field represents a scene as the volume density and directional emitted radiance at any point in space. We render the color of any ray passing through the scene using principles from classical volume rendering[1]. The volume density σ(x)\\sigma(\\mathbf x)σ(x) can be interpreted as the differential probability[2] of a ray terminating at a particle at location x\\mathbf xx. The expected color C(r)C(\\mathbf r)C(r) of camera ray r(t)\\mathbf r (t)r(t) with near bound tnt_ntn​ and far bound tft_ftf​. C(r)=∫tntfT(t)σ(r(t))c(r(t),d)dt C(\\mathbf r) = \\int_{t_n}^{t_f}T(t)\\sigma(\\mathbf{r}(t))\\mathbf{c}(\\mathbf{r}(t),\\mathbf{d})dt C(r)=∫tn​tf​​T(t)σ(r(t))c(r(t),d)dt r(t)=o+td\\mathbf r(t)=\\mathbf o+t\\mathbf dr(t)=o+td: Camera ray, where o\\mathbf oo is the position of the camera, ttt is the position of the point in the 3D space being rendered, and d\\mathbf dd is the direction of the camera ray. T(t)=exp⁡(−∫tntσ(r(s))ds)T(t) = \\exp{(-\\int_{t_n}^{t}\\sigma(\\mathbf{r}(s))ds)}T(t)=exp(−∫tn​t​σ(r(s))ds): Denotes the accumulated transmittance along the ray from tnt_ntn​ to ttt, i.e., the probability that the ray travels from tn to t without hitting any other particle. Example: In (a) and (b), we show the appearance of two fixed 3D points from two different camera positions: one on the side of the ship (orange insets) and one on the surface of the water (blue insets). Our method predicts the changing appearance of these two 3D points with respect to the direction of observation d\\mathbf dd, and in (c) we show how this behavior generalizes continuously across the whole hemisphere of viewing directions(This hemisphere can be viewed as the plot of (c,d)(\\mathbf c,\\mathbf d)(c,d), where d\\mathbf dd is the unit vector in the spherical coordinate frame and ccc shows the color). Discrete SamplingRendering a view from our continuous neural radiance field requires estimating this integral C(r)C(r)C(r) for a camera ray traced through each pixel of the desired virtual camera. However, MLP would only be queried at a discrete set of locations. So we use deterministic quadrature[3] to numerically estimate this continuous integral. we partition [tn,tf][t_n,t_f][tn​,tf​] into NNN evenly-spaced bins and then draw one sample uniformly at random from within each bin: ti∼U[tn+i−1N(tf−tn), tn+iN(tf−tn)] t_i \\sim \\mathcal{U} \\left[ t_n + \\frac{i-1}{N}(t_f-t_n),\\,\\, t_n + \\frac{i}{N}(t_f-t_n) \\right]\\ ti​∼U[tn​+Ni−1​(tf​−tn​),tn​+Ni​(tf​−tn​)] From Scene to Object: Estimation of C(r)C(\\mathbf r)C(r)(ci,ri)→C^(r)=∑i=1NTi(1−exp⁡(−σiδi))ci (c_i,r_i)\\rightarrow \\hat C(\\mathbf r)=\\sum_{i=1}^{N}T_i (1-\\exp({-\\sigma_i \\delta_i})) \\mathbf{c}_i (ci​,ri​)→C^(r)=i=1∑N​Ti​(1−exp(−σi​δi​))ci​ Ti=exp⁡(−∑j=1i−1σjδj) T_i=\\exp(- \\sum_{j=1}^{i-1} \\sigma_j \\delta_j) Ti​=exp(−j=1∑i−1​σj​δj​) δi=ti+1−ti\\delta_i=t_{i+1}-t_iδi​=ti+1​−ti​ : The distance between adjacent samples This function for calculating C^(r)\\hat{C}(\\mathbf{r})C^(r) from the set of (ci,σi)(\\mathbf{c}_i, \\sigma_i)(ci​,σi​) values is trivially differentiable and reduces to traditional alpha compositing[4] with alpha values αi=1−exp⁡(−σiδi)\\alpha_i = 1-\\exp(-\\sigma_i \\delta_i)αi​=1−exp(−σi​δi​). Implementation detailsNetwork Architecture First 888 layers (ReLU): Input: 3D coordinate x\\mathbf{x}x processed by γ\\gammaγ Output: σ\\sigmaσ ; 256-dimensional feature vector. 9th9^{th}9th layer: Input: σ\\sigmaσ ; 256-dimensional feature vector; Cartesian viewing direction unit vector d\\mathbf{d}d processed by γ\\gammaγ Output: View-dependent RGB color Details of variables are in Improving Scenes of High Frequency. Training Datasets: Captured RGB images of the scene, The corresponding camera poses and intrinsic parameters, and Scene bounds (we use ground truth camera poses, intrinsics, and bounds for synthetic data, and use the COLMAP structure-from-motion package to estimate these parameters for real data) Iteration: Randomly sample a batch of camera rays from the set of all pixels in the dataset following the hierarchical sampling Loss: The total squared error between the rendered and true pixel colors for both the coarse and fine renderings L=∑r∈Rays[∣∣C^c(r)−C(r)∣∣22+∣∣C^f(r)−C(r)∣∣22] L=\\sum_{\\mathbf r\\in Rays}\\left[||{\\hat C_c(\\mathbf r)-C(\\mathbf r)||_2^2+||\\hat C_f(\\mathbf r)-C(\\mathbf r)}||_2^2\\right] L=r∈Rays∑​[∣∣C^c​(r)−C(r)∣∣22​+∣∣C^f​(r)−C(r)∣∣22​] In our experiments, we use a batch size of 4096 rays, each sampled at Nc=64N_c=64Nc​=64 coordinates in the coarse volume and Nf=128N_f=128Nf​=128 additional coordinates in the fine volume. We use the Adam optimizer with a learning rate that begins at 5×10−45 \\times 10^{-4}5×10−4 and decays exponentially to 5×10−55 \\times 10^{-5}5×10−5 over the course of optimization (other Adam hyper-parameters are left at default values of β1=0.9\\beta_1=0.9β1​=0.9, β2=0.999\\beta_2=0.999β2​=0.999, and ϵ=10−7\\epsilon=10^{-7}ϵ=10−7). The optimization for a single scene typically take around 100–300k iterations to converge on a single NVIDIA V100 GPU (about 1–2 days). Notable Tricks Improving Scenes of High FrequencyDeep networks are biased towards learning lower frequency functions. findings in the context of neural scene representations, and show that reformulating FΘF_\\ThetaFΘ​ as a composition of two functions FΘ=FΘ′∘γ{F_\\Theta = F&#x27;_\\Theta \\circ \\gamma }FΘ​=FΘ′​∘γ, where γ\\gammaγ is fixed. It is used to map variables of RRR to R2LR^{2L}R2L. γ(p)=(sin⁡(20πp),cos⁡(20πp),⋯ ,sin⁡(2L−1πp),cos⁡(2L−1πp)) \\gamma(p) = \\left( \\begin{array}{ccccc} \\sin\\left(2^0 \\pi p\\right), &amp; \\cos\\left(2^0 \\pi p\\right), &amp; \\cdots, &amp; \\sin\\left(2^{L-1} \\pi p\\right), &amp; \\cos\\left(2^{L-1} \\pi p\\right) \\end{array} \\right) γ(p)=(sin(20πp),​cos(20πp),​⋯,​sin(2L−1πp),​cos(2L−1πp)​) This function γ(⋅)\\gamma(\\cdot)γ(⋅) is applied separately to each of the three coordinate values in x\\mathbf{x}x (which are normalized to lie in [−1,1][-1, 1][−1,1]) and to the three components of the Cartesian viewing direction unit vector d\\mathbf{d}d (which by construction lie in [−1,1][-1,1][−1,1]). In the experiments, we set L=10L=10L=10 for γ(x)\\gamma(\\mathbf x)γ(x) and L=4L=4L=4 for γ(d)\\gamma(\\mathbf d)γ(d). Reducing the Cost with Hierarchical SamplingOur rendering strategy of densely evaluating the neural radiance field network at NNN query points along each camera ray is inefficient: free space and occluded regions that do not contribute to the rendered image are still sampled repeatedly. Instead of just using a single network to represent the scene, we simultaneously optimize two networks: one “coarse’’ and one “fine’’. The coarse NetworkRewrite the alpha composited color as a weighted sum of all sampled colors cic_ici​ along the ray: C^c(r)=∑i=1Ncωici , ωi=Ti(1−exp⁡(−σiδi)) \\hat C_c(\\mathbf r)=\\sum_{i=1}^{N_c}\\omega_i c_i\\,, \\quad\\,\\, \\omega_i = T_i(1-\\exp(-\\sigma_i \\delta_i)) C^c​(r)=i=1∑Nc​​ωi​ci​,ωi​=Ti​(1−exp(−σi​δi​)) NcN_cNc​: The number of sampling points for coarse network. The fine NetworkNormalizing ωi\\omega_iωi​ as ω^i=ωi/∑j=1Ncωj\\hat \\omega_i ={\\omega_i}/{\\sum_{j=1}^{N_c} \\omega_j}ω^i​=ωi​/∑j=1Nc​​ωj​ produces a piecewise-constant PDF along the ray. Then sample from NfN_fNf​ locations from this distribution using inverse transform sampling[5]. Then we evaluate C^f(r)\\hat C_f(\\mathbf r)C^f​(r) using Nc+NfN_c+N_fNc​+Nf​ samples. ConclusionTODO Explanations [1] Volume rendering is a technique used in computer graphics and computer vision to visualize 3D data sets as 2D images. It works by slicing the 3D data set into a series of thin layers, and then rendering each layer as a 2D image from a specific viewpoint. These 2D images are then composited together to form the final volume rendering. [2] If a distribution (here in 3D space) has a density f(x,y,z)f(x,y,z)f(x,y,z), that means that for (almost) any volume in that space VVV, you can assign a probability to it by integrating the density (here “density” means probability per unit volume, very similar to, say, the concentration of salt in a solution). [3] Deterministic quadrature is a mathematical method used to estimate the definite integral of a function. The basic idea is to divide the area under the curve into smaller areas, and calculate the approximate value of the definite integral by summing the areas of the smaller areas. There are several types of deterministic quadrature methods, including the trapezoidal rule, Simpson’s rule, and Gaussian quadrature. [4] Alpha compositing is a technique used in computer graphics and image processing to combine two or more images or video frames by blending them together using an alpha channel. The alpha channel is a mask that defines the transparency or opacity of each pixel in the image. Alpha compositing is used to create composites, where the resulting image is a combination of the original images, with the transparency or opacity of each image controlled by the alpha channel. The alpha channel can be used to create effects such as blending, fading, and layering. Alpha compositing - Wikipedia [5] Inverse transform sampling (ITS) is a technique used in digital signal processing to reconstruct a signal from a set of samples. It is the inverse of the discrete Fourier transform(DFT). The basic idea behind ITS is to use the Fourier coefficients obtained from DFT to reconstruct the signal in the time domain. Inverse transform sampling - Wikipedia"},{"title":"【笔记】项目管理的逻辑","date":"2022-08-22T00:52:57.000Z","url":"/Project-Management/pro-mgt-lecs.html","tags":[["项目管理","/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"]],"categories":[["Project-Management","/categories/Project-Management/"]],"content":"本笔记主要记录清华大学管理学系列讲座相关笔记。 什么是项目项目与运营：团队的两大任务运营(Operation)：持续性、重复性的工作，如报税、报销、记账等 项目(Project)：阶段性，一次性的工作 项目是为创造独特的产品、服务或成果进行的临时性工作 ——PMBOK 6th 项目的特点 组织项目管理 项目生命周期与应用项目生命周期主要有以下几种： 预测型 迭代型 增量型 敏捷型 预测型 在做项目前已对项目结果心中有数 结果明确，开发过程成熟 Example 经典瀑布开发模型 每一步需要很完善，很到位 需求分析阶段随便提需求，但开始方案设计后不能再动（项目进行方向不可逆） 迭代型与增量型迭代型：项目为一整体，逐版本迭代升级 增量型：整个项目分为多个部分，逐个交付，每个交付都是完成状态 客户可提前了解工作成果 适应性开发（敏捷开发） Product Backlog: 客户需求池 Sprint: 开发周期 有节奏地，持续性地进行适应性开发。 综合对比 管控项目进度 需求不断变更，时间、物质成本不足：先上线，后迭代？ ⟹ \\implies⟹ 运维不便 应当确定合理的项目阶段划分 项目的阶段划分Example 1. 工程建设 按照专业每个阶段设置关口，及时验收，满足条件才可进入下一阶段 Example 2. 产品设计主要流程： 需求分析 → 原型设计 → 产品开发 → 验收交付 控制项目质量；发现脱离初衷&#x2F;难以修正时及时止损 主要看是否实现项目既定目标。 项目的分工面向项目的管理模式以矩阵形式为特征： 既有按职能划分的专业部门，也有从专业部门抽调形成的项目团队 竖向划分是专业，职能部门；横向划分是项目团队 项目的相关方凸显模型 (Salience Model) (蓝色为所具备特征) 取得项目相关方共识 References【公开课】清华大学：项目管理的逻辑（全6讲）"},{"title":"你好，世界！","date":"1970-01-01T00:00:01.000Z","url":"/uncategorized/hello-world.html","categories":[["undefined",""]],"content":"欢迎使用 Kratos : Rebirth 这个我们精心打造的 Hexo 主题！希望能在接下来的旅途中与您相伴，共同创造出更多难以忘怀的美好体验。"},{"title":"test_formula","date":"1970-01-01T00:00:01.000Z","url":"/uncategorized/test-formula.html","categories":[["undefined",""]],"content":"Latex testsHere are various LaTeX formulas to test rendering capabilities: Inline FormulasEinstein’s famous equation: E=mc2E = mc^2E=mc2 shows the relationship between energy and mass. The quadratic formula x=−b±b2−4ac2ax = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}x=2a−b±b2−4ac​​ solves ax2+bx+c=0ax^2 + bx + c = 0ax2+bx+c=0. The Pythagorean theorem states that a2+b2=c2 a^2 + b^2 = c^2 a2+b2=c2 for right triangles. Centered EquationsNewton’s second law of motion: F=m⋅a F = m \\cdot a F=m⋅a Euler’s identity, often considered the most beautiful equation in mathematics: eiπ+1=0 e^{i\\pi} + 1 = 0 eiπ+1=0 The Gaussian integral: ∫−∞∞e−x2dx=π\\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}∫−∞∞​e−x2dx=π​ Begin&#x2F;End BlocksA matrix representation: abcdefghi \\begin{array}{ccc} a &amp; b &amp; c \\\\ d &amp; e &amp; f \\\\ g &amp; h &amp; i \\end{array} adg​beh​cfi​ A system of linear equations: 3x+2y−z=12x−2y+4z=−2−x+12y−z=0 \\begin{aligned} 3x + 2y - z &amp;= 1 \\\\ 2x - 2y + 4z &amp;= -2 \\\\ -x + \\frac{1}{2}y - z &amp;= 0 \\end{aligned} 3x+2y−z2x−2y+4z−x+21​y−z​=1=−2=0​ A piecewise function definition: {x2if x≥0−x2if x&lt;0 \\begin{cases} x^2 &amp; \\text{if } x \\geq 0 \\\\ -x^2 &amp; \\text{if } x &lt; 0 \\end{cases} {x2−x2​if x≥0if x&lt;0​ Left&#x2F;Right DelimitersThe expectation value in quantum mechanics: ⟨ψ∣A^∣ψ⟩\\left\\langle \\psi \\right| \\hat{A} \\left| \\psi \\right\\rangle⟨ψ∣A^∣ψ⟩ A binomial coefficient: (nk)=(nk)=n!k!(n−k)!\\binom{n}{k} = \\left( \\begin{array}{c} n \\\\ k \\end{array} \\right) = \\frac{n!}{k!(n-k)!}(kn​)=(nk​)=k!(n−k)!n!​ The Legendre symbol: (ap)={1if a is a quadratic residue modulo p and a≢0(modp)−1if a is a quadratic non-residue modulo p0if a≡0(modp) \\left( \\frac{a}{p} \\right) = \\begin{cases} 1 &amp; \\text{if } a \\text{ is a quadratic residue modulo } p \\text{ and } a \\not\\equiv 0 \\pmod{p} \\\\ -1 &amp; \\text{if } a \\text{ is a quadratic non-residue modulo } p \\\\ 0 &amp; \\text{if } a \\equiv 0 \\pmod{p} \\end{cases} (pa​)=⎩⎨⎧​1−10​if a is a quadratic residue modulo p and a≡0(modp)if a is a quadratic non-residue modulo pif a≡0(modp)​ Complex ExpressionsThe Fourier transform: F[f(t)]=∫−∞∞f(t)e−iωtdt\\mathcal{F}[f(t)] = \\int_{-\\infty}^{\\infty} f(t) e^{-i\\omega t} dtF[f(t)]=∫−∞∞​f(t)e−iωtdt Schrödinger’s equation: iℏ∂∂tΨ(r,t)=[−ℏ22m∇2+V(r,t)]Ψ(r,t)i\\hbar\\frac{\\partial}{\\partial t}\\Psi(\\mathbf{r},t) = \\left [ -\\frac{\\hbar^2}{2m}\\nabla^2 + V(\\mathbf{r},t)\\right ] \\Psi(\\mathbf{r},t)iℏ∂t∂​Ψ(r,t)=[−2mℏ2​∇2+V(r,t)]Ψ(r,t) Maxwell’s equations in differential form: ∇⋅E=ρε0∇⋅B=0∇×E=−∂B∂t∇×B=μ0J+μ0ε0∂E∂t \\begin{aligned} \\nabla \\cdot \\mathbf{E} &amp;= \\frac{\\rho}{\\varepsilon_0} \\\\ \\nabla \\cdot \\mathbf{B} &amp;= 0 \\\\ \\nabla \\times \\mathbf{E} &amp;= -\\frac{\\partial\\mathbf{B}}{\\partial t} \\\\ \\nabla \\times \\mathbf{B} &amp;= \\mu_0\\mathbf{J} + \\mu_0\\varepsilon_0\\frac{\\partial\\mathbf{E}}{\\partial t} \\end{aligned} ∇⋅E∇⋅B∇×E∇×B​=ε0​ρ​=0=−∂t∂B​=μ0​J+μ0​ε0​∂t∂E​​ More info: Deployment"},{"title":"My Supportive Pals","date":"2022-06-04T00:00:01.000Z","url":"/friends/index.html","categories":[["undefined",""]],"content":" (() => { const flist = [{\"title\":\"TingNote\",\"description\":\"喵~\",\"image\":\"../images/ting.png\",\"link\":\"\"},{\"title\":\"欢迎来看红光今天吃什么\",\"description\":\"喵~\",\"image\":\"\",\"link\":\"\"},{\"title\":\"木头人的小木屋\",\"description\":\"人生南北多歧路，时代喧哗造物忙。\",\"image\":\"\",\"link\":\"\"}]; let friendNodes = ''; while (flist.length > 0) { const randID = Math.floor(Math.random()*flist.length); friendNodes += ` ${flist[randID].title} ${flist[randID].description} `; flist.splice(randID, 1); } document.currentScript.parentNode.querySelector(\".kr-linklist-container\").innerHTML = friendNodes; })(); "}]